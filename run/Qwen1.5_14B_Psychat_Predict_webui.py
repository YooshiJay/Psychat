#!/usr/bin/env python
# coding: utf-8

# In[1]:


# ## import pandas
# # print(pandas.__version__)

# !pip install -q bitsandbytes datasets accelerate loralib
# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git
# !pip install transformers_stream_generator einops
# !pip install openai
# !pip install cohere
# !pip install tiktoken
# !pip install gradio

# !pip uninstall transformers -y
# !pip install transformers==4.36.0


# In[2]:


class args:
    max_seq_length = 1024
    model_name = "/gemini/data-2"
    peft_model_id = "/gemini/pretrain"
    prompt = "ä½ æ˜¯ä¸€ä½å¿ƒç†å’¨è¯¢å¸ˆï¼Œç°åœ¨ä»¥æ¸©æš–äº²åˆ‡çš„è¯­æ°”ï¼Œä¸æ¥è®¿è€…è¿›è¡Œå¯¹è¯ï¼Œè¯·ä¸»åŠ¨å¤šæ¬¡è¯¢é—®æ¥è®¿è€…çš„å†…å¿ƒè¯‰æ±‚ï¼Œ\
              æ›´æ³¨é‡å…±æƒ…å’Œå°Šé‡æ¥è®¿è€…çš„æ„Ÿå—ã€‚æ ¹æ®æ¥è®¿è€…çš„åé¦ˆè°ƒæ•´å›åº”ï¼Œç¡®ä¿å›åº”è´´åˆæ¥è®¿è€…çš„æƒ…å¢ƒå’Œéœ€æ±‚ã€‚"


# In[3]:


import torch
from peft import PeftModel, prepare_model_for_kbit_training
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4")

model = AutoModelForCausalLM.from_pretrained(
    args.model_name,
    device_map='auto',
    return_dict=True,
    quantization_config=quantization_config,
)

tokenizer = AutoTokenizer.from_pretrained(args.model_name)
tokenizer.im_start_id = tokenizer.encode('<|im_start|>')[0]
tokenizer.im_end_id = tokenizer.encode('<|im_end|>')[0]
tokenizer.eos_token = tokenizer.pad_token

model = prepare_model_for_kbit_training(model)

# Load the Lora model
model = PeftModel.from_pretrained(model, args.peft_model_id)
model.eval()


# In[4]:


import pandas as pd
import torch
from peft import PeftModel, PeftConfig, prepare_model_for_kbit_training
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from transformers import BitsAndBytesConfig


def generate_and_tokenize_prompt_for_interface(ask, history):
    system = args.prompt

    system_text = f'<|im_start|>system\n{system}<|im_end|>\n'
    input_ids = tokenizer.encode(system_text, add_special_tokens=False)

    # æ‹¼æ¥å¤šè½®å¯¹è¯
    for i, conv in enumerate(history):
        human = conv[0].strip()
        assistant = conv[1].strip()

        input_tokens = tokenizer.encode(f'<|im_start|>user\n{human}<|im_end|>\n', add_special_tokens=False)
        output_tokens = tokenizer.encode(f'<|im_start|>assistant\n{assistant}<|im_end|>' + tokenizer.eos_token + '\n', add_special_tokens=False)

        input_ids += input_tokens + output_tokens

    # æ‹¼æ¥å½“å‰æé—®
    input_ids += tokenizer.encode(f'<|im_start|>user\n{ask}<|im_end|>\n<|im_start|>assistant\n', add_special_tokens=False)

    # å¯¹é•¿åº¦è¿›è¡Œæˆªæ–­
    input_ids = input_ids[-args.max_seq_length:]
    input_ids = torch.tensor([input_ids]).cuda()
    return input_ids

###
# è¾“å…¥:
# data:é«˜ä¸‰åçš„è¿·èŒ«ï¼Œé«˜è€ƒå‰çš„ææƒ§ï¼Œèƒ½ç»™æˆ‘ä¸€äº›å»ºè®®ä¹ˆï¼Ÿ
# history:[{"human": "ä½ å¥½å‘€", "assistant": "ä½ å¥½ï¼Œæˆ‘æ˜¯xxxï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡"}, ...]

generation_config = GenerationConfig(
        temperature=0.35,
        top_p = 0.9,
        repetition_penalty=1.0,
        max_new_tokens=500,  # max_length=max_new_tokens+input_sequence
        # min_new_tokens = 1,
        do_sample = True,
#         eos_token_id=tokenizer.eos_token_id
)


# In[5]:


import gradio as gr
import time

def generate_answer(human, history):
    input_ids = generate_and_tokenize_prompt_for_interface(human, history)

    with torch.no_grad():
        s = model.generate(input_ids=input_ids, generation_config=generation_config)

    output = tokenizer.decode(s[0], skip_special_tokens=True)
    assistant = output[output.rfind("assistant")+10:]
    return assistant

def slow_echo(message, history):
    # å…¶ä¸­messageæ¥è‡ªäºå‰ç«¯ç•Œé¢çš„è¾“å…¥ä¿¡æ¯ï¼Œå³ç”¨æˆ·çš„æé—®
    # historyä¸ºå†å²é—®ç­”è®°å½•ï¼Œæ­¤å¤„å¯ä»¥å¿½ç•¥
    # textä¹Ÿæ¥è‡ªäºå‰ç«¯ç•Œé¢çš„è¾“å…¥ä¿¡æ¯ï¼Œä¸ºå‚è€ƒæ–‡æœ¬
    answer = generate_answer(message, history)
    for i in range(len(answer)):
        time.sleep(0.05)
        yield answer[: i+1]

# demo2 = gr.ChatInterface(
#     slow_echo,
# ).queue()
#   # æˆ‘ä»¬ä¸å¼ºåˆ¶è¦æ±‚ä½ åšæˆæµè¾“å‡ºçš„æ ·å¼ï¼Œåªè¦èƒ½å¤Ÿå¾—åˆ°å›ç­”å³å¯ã€‚

# title = "ğŸ’– å®‰å¿ƒè½æ„â€”â€”ä¸“æ³¨å¿ƒç†å’¨è¯¢çš„å¤§è¯­è¨€æ¨¡å‹"

with gr.Blocks() as demo:
    # åˆ›å»ºä¸€ä¸ªMarkdownç»„ä»¶ï¼Œç”¨æ¥æ˜¾ç¤ºæç¤ºä¿¡æ¯
    gr.Markdown('<h1 style="color: #333; font-size: 24px; margin-bottom: 20px; text-align: center;">ğŸ’– å®‰å¿ƒè½æ„â€”â€”ä¸“æ³¨å¿ƒç†å’¨è¯¢çš„å¤§è¯­è¨€æ¨¡å‹</h1>')
    gr.Markdown('<p style="font-family: Arial;">\
                     ğŸ”¥ æˆ‘ä»¬çš„å¿ƒç†å¯¹è¯æ¨¡å‹æ—¨åœ¨å¸®åŠ©é‚£äº›é¢ä¸´æƒ…ç»ªå›°æ‰°ã€å¿ƒç†å‹åŠ›æˆ–äººé™…äº¤å¾€å›°éš¾çš„ä¸ªäººã€‚</p>')
    gr.Markdown('<p style="font-family: Arial;">\
                     â­ é€šè¿‡ä¸æˆ‘ä»¬çš„æ¨¡å‹äº’åŠ¨ï¼Œä»–ä»¬å¯ä»¥è·å¾—æƒ…ç»ªæ”¯æŒã€å¿ƒç†æŒ‡å¯¼å’Œè§£å†³é—®é¢˜çš„æ–¹æ³•ï¼Œä»è€Œé‡æ‹¾å†…å¿ƒçš„å¹³è¡¡ä¸è‡ªä¿¡ï¼Œå®ç°æ›´åŠ å¥åº·ã€ç§¯æçš„ç”Ÿæ´»æ–¹å¼ã€‚</p>')
    gr.Markdown('<p style="font-family: Arial;">\
                     ğŸ¥° è¯·æ”¾å¿ƒä¸æˆ‘ä»¬çš„æ¨¡å‹å¯¹è¯ï¼Œæˆ‘ä»¬éƒ‘é‡æ‰¿è¯ºä¸ä¼šè·å–ä»»ä½•ä¸ªäººéšç§ä¿¡æ¯ï¼</p>')
    gr.Markdown('<p style="font-family: Arial;">\
                     â­• è¯·æ³¨æ„ï¼šå¤§æ¨¡å‹åªæä¾›ç®€å•å¸®åŠ©ï¼Œå¦‚æœ‰å¿…è¦ï¼Œè¯·è”ç³»ä¸“ä¸šå¿ƒç†å¸ˆï¼æ¨¡å‹åœ¨è®­ç»ƒæ—¶éå¸¸æ³¨æ„ä»·å€¼è§‚å¼•å¯¼ï¼Œä½†æˆ‘ä»¬ä¸èƒ½ä¿è¯å…¶è¾“å‡ºå§‹ç»ˆæ­£ç¡®ä¸”æ— å®³ã€‚åœ¨å†…å®¹æ–¹é¢ï¼Œæ¨¡å‹çš„ä½œè€…å’Œå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚</p>')
    interface = gr.ChatInterface(slow_echo, 
                                 # title=title
                                ).queue()
    # blocks = gr.Blocks([demo2, md])

if __name__ == "__main__":
    model.eval()
    demo.launch(server_name='127.0.0.1', server_port=7860)


# In[ ]:




