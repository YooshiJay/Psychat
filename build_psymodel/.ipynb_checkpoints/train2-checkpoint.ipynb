{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1ec743-a33f-4d4b-82e7-e8c555375121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    max_seq_length = 1024\n",
    "    model_name = \"/gemini/data-2\"\n",
    "    train_file = \"/gemini/data-3/vicunaformatfixedfinal.json\"\n",
    "    # output_file = \"/gemini/code/build_psymodel/E_output\"\n",
    "    output_file = \"/gemini/E_output\"\n",
    "    prompt = \"You're a psychotherapist, and you're engaging in a conversation with visitors in a warm and welcoming manner.\" \\\n",
    "             \"You're encouraged to inquire multiple times about the visitors' inner desires, emphasizing empathy and respecting their feelings.\" \\\n",
    "             \"Adjust your responses based on their feedback, ensuring they align with the visitors' circumstances and needs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769c5cda-e8b1-4b65-a1b7-e974c1f65e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13696, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13696, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13696, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name) # trust_remote_code=True\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7691ec8-4006-4d98-a472-24cd84919c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.im_start_id = tokenizer.encode('<|im_start|>')[0]\n",
    "tokenizer.im_end_id = tokenizer.encode('<|im_end|>')[0]\n",
    "tokenizer.eos_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3bafb4-ad4d-4123-b09b-a2240de3a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, #attention heads\n",
    "    lora_alpha=16, #alpha scaling\n",
    "    # target_modules=[\"c_attn\",\"c_proj\",\"w1\",\"w2\"],\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Load the Lora model\n",
    "# model = PeftModel.from_pretrained(model, \"yooshijay/qwen-14b-version2\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7db0113-d651-479e-8e1d-b612a51d6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Any, Dict, List\n",
    "import json\n",
    "\n",
    "class QwenSFTDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_seq_length, prompt):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.im_start_id = tokenizer.im_start_id\n",
    "        self.im_end_id = tokenizer.im_end_id\n",
    "        self.enter_token_ids = tokenizer.encode('\\n')   # 回车键\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.prompt = prompt\n",
    "        # logger.info('Loading data: {}'.format(file))\n",
    "        with open(file, 'r', encoding='utf8') as f:\n",
    "            data_list = json.load(f)\n",
    "\n",
    "        # logger.info(\"there are {} data in dataset\".format(len(data_list)))\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        数据拼接格式如下：\n",
    "        <|im_start|>system\n",
    "        You are a helpful assistant.<|im_end|>\n",
    "        <|im_start|>user\n",
    "        你好呀<|im_end|>\n",
    "        <|im_start|>assistant\n",
    "        你好，我是xxx，很高兴为您服务<|im_end|><|endoftext|>\n",
    "        \"\"\"\n",
    "        data = self.data_list[index]\n",
    "        # data = json.loads(data)\n",
    "        if 'system' in data.keys():\n",
    "            system = data['system'].strip()\n",
    "        else:\n",
    "            system = self.prompt\n",
    "        conversations = data['conversations']\n",
    "\n",
    "        # 收集模型输入\n",
    "        system_text = f'<|im_start|>system\\n{system}<|im_end|>\\n'\n",
    "        input_ids = self.tokenizer.encode(system_text, add_special_tokens=False)\n",
    "        target_mask = [0] * len(input_ids)\n",
    "\n",
    "        # 拼接多轮对话\n",
    "        for i, conv in enumerate(conversations):\n",
    "            if conv['from'] == 'human':\n",
    "                human = conv['value'].strip()\n",
    "                input_tokens = self.tokenizer.encode(f'<|im_start|>user\\n{human}<|im_end|>\\n', add_special_tokens=False)\n",
    "                input_ids += input_tokens\n",
    "                # input_tokens部分不计算loss\n",
    "                target_mask += [0] * len(input_tokens)\n",
    "            else:\n",
    "                assistant = conv['value'].strip()\n",
    "                output_tokens = self.tokenizer.encode(f'<|im_start|>assistant\\n{assistant}<|im_end|>' + tokenizer.eos_token + '\\n', add_special_tokens=False)\n",
    "                input_ids += output_tokens\n",
    "                # '<|im_start|>assistant\\n'占3个token，结尾的'\\n'占1个token，不计算它们的loss\n",
    "                target_mask += [0] * 3 + [1] * (len(output_tokens) - 4) + [0] \n",
    "\n",
    "        assert len(input_ids) == len(target_mask)\n",
    "        # 对长度进行截断\n",
    "        input_ids = input_ids[:self.max_seq_length]\n",
    "        target_mask = target_mask[:self.max_seq_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        assert len(input_ids) == len(target_mask) == len(attention_mask)\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'target_mask': target_mask\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "class SFTDataCollator(object):\n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # 找出batch中的最大长度\n",
    "        lengths = [len(x['input_ids']) for x in batch if x['input_ids'] is not None]\n",
    "        # 取出batch中的最大长度，如果超过max_seq_length，则取max_seq_length\n",
    "        batch_max_len = min(max(lengths), self.max_seq_length)\n",
    "        # batch_max_len = self.max_seq_length\n",
    "\n",
    "        input_ids_batch, attention_mask_batch, target_mask_batch = [], [], []\n",
    "        # truncate and padding\n",
    "        for x in batch:\n",
    "            input_ids = x['input_ids']\n",
    "            attention_mask = x['attention_mask']\n",
    "            target_mask = x['target_mask']\n",
    "            if input_ids is None:\n",
    "                # logger.info('some input_ids is None')\n",
    "                continue\n",
    "            padding_len = batch_max_len - len(input_ids)\n",
    "            # padding\n",
    "            input_ids = input_ids + [self.pad_token_id] * padding_len\n",
    "            attention_mask = attention_mask + [0] * padding_len\n",
    "            target_mask = target_mask + [0] * padding_len\n",
    "            # truncate\n",
    "            input_ids = input_ids[:self.max_seq_length]\n",
    "            attention_mask = attention_mask[:self.max_seq_length]\n",
    "            target_mask = target_mask[:self.max_seq_length]\n",
    "\n",
    "            input_ids_batch.append(input_ids)\n",
    "            attention_mask_batch.append(attention_mask)\n",
    "            target_mask_batch.append(target_mask)\n",
    "\n",
    "        # 将list转换为tensor，得到最终的的模型输入\n",
    "        input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)\n",
    "        attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long)\n",
    "        target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long)\n",
    "\n",
    "        labels = torch.where(target_mask_batch == 1, input_ids_batch, -100)\n",
    "        inputs = {\n",
    "            'input_ids': input_ids_batch,\n",
    "            'attention_mask': attention_mask_batch,\n",
    "            'labels': labels\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "train_dataset = QwenSFTDataset(args.train_file, tokenizer, args.max_seq_length, args.prompt)\n",
    "data_collator = SFTDataCollator(tokenizer, args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656861b-3bf4-4b94-bf51-dab05219afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='6192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  32/6192 35:06 < 120:07:15, 0.01 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.955500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (DataCollatorForSeq2Seq, Trainer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        # warmup_steps=100,\n",
    "        # max_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir='temp',\n",
    "        # output_dir='/gemini/code/build_psymodel/temp',\n",
    "        num_train_epochs=2,\n",
    "        remove_unused_columns = False\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "model.save_pretrained(args.output_file)\n",
    "\n",
    "# model.push_to_hub(\"yooshijay/qwen-14b-version4\",\n",
    "#                   use_auth_token=True,\n",
    "#                   commit_message=\"basic training\",\n",
    "#                   private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a03850-2d12-4bf8-8d0d-dc8eadee22c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
